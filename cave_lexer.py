from cave_classes import *

# toToken :: Tuple[int,str] -> int -> Token
@dcDecorator
def toToken(keyword_tup: Tuple[int,str], linenr: int) -> Token:
    """ This function creates a token from a keyword, its position on the line and its line number. """
    match_input = keyword_tup[1]
    match match_input:
        case "plooga":
            return Token(TokenTypes.ADD, "plooga", (linenr, keyword_tup[0]))
        case "mooga":
            return Token(TokenTypes.SUB, "mooga", (linenr, keyword_tup[0]))
        case "kooga":
            return Token(TokenTypes.MUL, "kooga", (linenr, keyword_tup[0]))
        case "dooga":
            return Token(TokenTypes.DIV, "dooga", (linenr, keyword_tup[0]))
        case "isooga":
            return Token(TokenTypes.ASSIGN, "isooga", (linenr, keyword_tup[0]))
        case "ifooga":
            return Token(TokenTypes.IF, "ifooga", (linenr, keyword_tup[0]))
        case "elooga":
            return Token(TokenTypes.ELSE, "elooga", (linenr, keyword_tup[0]))
        case "whooga":
            return Token(TokenTypes.WHILE, "whooga", (linenr, keyword_tup[0]))
        case "eqooga":
            return Token(TokenTypes.EQUALS, "eqooga", (linenr, keyword_tup[0]))
        case "greqooga":
            return Token(TokenTypes.GREQ, "greqooga", (linenr, keyword_tup[0]))
        case "leeqooga":
            return Token(TokenTypes.LEEQ, "leeqooga", (linenr, keyword_tup[0]))
        case "lesooga":
            return Token(TokenTypes.LESSER, "lesooga", (linenr, keyword_tup[0]))
        case "grooga":
            return Token(TokenTypes.GREATER, "grooga", (linenr, keyword_tup[0]))
        case "neqooga":
            return Token(TokenTypes.NOTEQUAL, "neqooga", (linenr, keyword_tup[0]))
        case "booga":
            return Token(TokenTypes.END, "booga", (linenr, keyword_tup[0]))
        case "komooga":
            return Token(TokenTypes.SEP, "komooga", (linenr, keyword_tup[0]))
        case "hoooga":
            return Token(TokenTypes.OPENPAR, "hoooga", (linenr, keyword_tup[0]))
        case "hsooga":
            return Token(TokenTypes.CLOSEPAR, "hsooga", (linenr, keyword_tup[0]))
        case "broooga":
            return Token(TokenTypes.OPENBR, "broooga", (linenr, keyword_tup[0]))
        case "brsooga":
            return Token(TokenTypes.CLOSEBR, "brsooga", (linenr, keyword_tup[0]))
        case "fdooga":
            return Token(TokenTypes.DEF, "fdooga", (linenr, keyword_tup[0]))
        case "retooga":
            return Token(TokenTypes.RETURN, "retooga", (linenr, keyword_tup[0]))
        case match_input if re.fullmatch("[a-zA-Z]\w*", match_input):
            return Token(TokenTypes.ID, keyword_tup[1], (linenr, keyword_tup[0]))
        case match_input if match_input.isdigit():
            return Token(TokenTypes.NUMBER, keyword_tup[1], (linenr, keyword_tup[0]))

# lineToKeywords :: str -> List[Tuple[int,str]]
@dcDecorator
def lineToKeywords(line: str) -> List[Tuple[int,str]]:
    """ This function splits lines from the original code file into keywords that can be used by the lexer.
        It returns a list of tuples containing the keyword and its position in a line.
        The position is given as the keywords position in the keyword list of a given line. """
    line = line.strip()
    line = line.split()
    enum_line = list(enumerate(line, 1))
    return enum_line


A = TypeVar('A')
B = TypeVar('B')
C = TypeVar('C')

# tokenMap :: Callable[[A, B], C] -> List[Tuple[int,str]] -> int -> List[Token]
@dcDecorator
def tokenMap(func: Callable[[A, B], C], keywords_list: List[Tuple[int,str]], linenr: int) -> List[Token]:
    """ This function is made to act like a map function but with the ability to give an extra parameter that stays constant.
        It is used to call the toToken function to turn a list of lines with keywords into a token list. """
    if len(keywords_list) == 0:
        return []
    else:
        head, *tail = keywords_list
        return [func(head, linenr)] + tokenMap(func, tail, linenr)


# keywordsToToken :: Tuple[int, List[Tuple[int,str]]] -> List[Token]
@dcDecorator
def keywordsToToken(keywords_list: Tuple[int,List[Tuple[int,str]]]) -> List[Token]:
    """ This function takes the list of keywords generated by the lineToKeywords function and turns it into a token list."""
    tokens_list = list(tokenMap(toToken, keywords_list[1], keywords_list[0]))
    return tokens_list


# flattenList :: List[List[Token]] -> List[Token]
@dcDecorator
def flattenList(token_list: List[List[Token]]) -> List[Token]:
    """ This function is used to flatten a list for easier use. """
    flat_list = reduce(lambda x, y: x + y, token_list)
    return flat_list


# caveLexer :: List[List[str]] -> Type[Enum] -> List[Token]
@dcDecorator
def caveLexer(code_text: List[List[str]], token_types: Type[Enum] = TokenTypes) -> List[Token]:
    """ This function turns a list of lines from a text file into a list of tokens.
        This list can then be used by the parser. """
    keywords = []
    keywords = list(enumerate(map(lineToKeywords, code_text), 1))
    token_list = list(map(keywordsToToken, keywords))
    flat_token_list = flattenList(token_list)
    return flat_token_list